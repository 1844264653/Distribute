### 第二章  单机存储系统

**单机存储引擎就是哈希表，B树等数据结构在机械磁盘，SSD等持久化介质上的实现**（**哈希存储引擎**是**哈希表的持久化实现**，**B树存储引擎**是**B树的持久化实现**，**LSM[Log Structure Merge Tree]树存储引擎采用批量转存储技术来避免磁盘随机写入**）

单机存储系统是单机存储引擎的一种封装，对外提供文件、键值，表格或者关系模型

单机存储系统的理论来源于关系数据库。

**数据库将一个或者多个操作组成一组**，**称作事务**，事务必须满足**ACID，分别是原子性Atomicity，一致性Consistency，隔离性Isolation，以及持久性Durability**

**多个事务并发执行时**，数据库的**并发控制器**必须能够保证多个事务的执行结果不能破坏某种约定，如不能出现事务执行到一半的情况，不能读取到未提交的事务，等等

为了保证持久性，对于数据库的每一个变化都要在磁盘上记录日志，当数据库系统突然发生故障，重启后能够恢复到之前一致的状态



##### 硬件基础

**摩尔定律**告诉我们，每18个月计算机等IT产品的性能会翻一番，或者说相同性能的IT产品，每18个月会降价一半。但是计算机的硬件体系架构相对保持稳定

**架构设计**很重要的一点就是合理的选择屏弃恶能够最大限度地发挥底层硬件的价值



##### cpu架构

早期cpu为**单核芯片**，但是提高单核的速度会产生过多的热量且**无法带来相应的性能改善**  ———— 设计成多核或者多个CPU

经典的多cpu架构为对称多处理结构——SMP ——Symmetric Multi-Precessing，即在一个计算机汇集了一组处理器，**它们之间对称工作，无主次或者从属关系，共享相同的物理内存以及总线**，如图



![](E:\auto\Distribute\large_scale_distributed_system_principles_and_architectures\SMP-system-structure.png)

​	图中的SMP系统由两个CPU构成，每个cpu有两个核心core，cpu与内存之间通过总线通信。每个核心有各自的L1d Cache——L1数据缓存，以及L1i Cache——L1指令缓存，同一个CPU的多个核心共享L2以及L3缓存，另外，某些CPU还可以通过**超线程技术Hyper-Threading Teachnology**使得 一个核心具有同时执行两个线程的能力。

​	SMP**架构的主要特征是共享**，系统中所有资源——CPU,内存,I/O等，都是共享的，由于多CPU**对前端总线的竞争**，SMP的拓展能力非常有限。为了提高拓展性，现在的主流服务器架构一般为NUMA架构——Non-Uniform Memory Access，非一致存储访问架构。它具有多个NUMA节点，每个NUMA节点是一个SMP结构，一般由多个CPU（如4个）组成，并且具有独立的本地内存、I/O槽口等。

​	NUMA节点可以直接快速访问本地内存，也可以通过NUMA**互联互通模块**访问其他NUMA节点的内存，**访问本地内存的速度远远高于远程访问的速度**。因此为了更好的发挥系统的性能，开发应用程序时**需要尽量减少不同NUMA节点之间的信息交互**

##### I/O总线

​	**存储系统的性能瓶颈一般在于IO**，因此有必要对IO子系统的架构有一个大致的了解！

如图：它是典型的南北桥架构。北桥芯片通过前端总线FSB与 CPU相连，内存模块以及PCI-E设备——如高端的SSD设备Fusion-IO，挂接在北桥上。北桥与南桥之间通过DMI连接，DMI的带宽为1GB/s，网卡——包括千兆以及万兆网卡，硬盘以及中低端固态硬盘——inter 320系列SSD，挂接在南桥上。如果采用SATA2接口，那么最大带宽为300MB/s

![](E:\auto\Distribute\large_scale_distributed_system_principles_and_architectures\Inter X48主板南北桥架构.jpg)







##### 网路拓扑

拓扑一般典型分为三层：**核心层（core）， 汇聚层（aggregation），接入层（edge）**

典型的接入层交换机包含48个1Gb端口以及4个10Gb上行端口，汇聚层以及核心层的交换机包含128个10Gb的端口

**传统问题**：第三层接入层可能有很多的交换机接到汇聚层，很多汇聚层交换机接入到核心层。

**同一个接入层下的服务器**之间的带宽为1Gb，**不同接入层交换机下的服务器**之间的带宽小于1Gb，由于**同一个接入的服务器往往部署在同一个机架内**，因此，设计系统的时候需要哦考虑服务器是否在一个机架内，**减少跨机架拷贝大量数据**。

例如：**Hadoop的HDFS默认存储三个副本，其中两个副本放在同一个机架**，就是这个原因

为了减少系统对网络拓扑结构的依赖，Google在2008年时候将网络改造为**扁平化**图谱结构——即**三级CLOS网络**，**同一个集群内最多支持20480台服务器**，且任**意两台**都有1Gb带宽。**CLOS网络需要额外投入更多的交换机**，带来的好处也是明显的，设计系统时不需要考虑底层网络拓扑，从而很**方便的将整个集群做成一个计算资源池**

**存储系性能的瓶颈主要在于磁盘随机读写**，设计存储引擎的时候会这针对磁盘特性做很多处理，比如将随机写操作转化成顺序写，通过缓存减少此怕随机读操作【自行了解SSD盘，SAS磁盘，SATA磁盘的差异】

整个集群中所有的服务器上的存储介质（内存，机械硬盘，SSD）构成一个整体，其他服务器上的存储介质与本机存储介质一样都是可以访问的，区别仅仅在于 需要**额外的网络传输以及网络协议栈**等访问开销。

**存储系统的性能主要包括两个维度：吞吐量以及访问延时**，设计系统是要求能够再保证访问延时的基础上，通过最低成本实现尽可能高的吞吐量。**磁盘和SSD的访问延时差别很大，但是带宽差别不打，因此磁盘适合大块顺序访问的存储系统，SSD适合随机访问较多或者对延时比较敏感的关键系统**

，二者也经常组合到哦一起进行混合存储，热数据【频繁访问】存储到SSD中，冷数据【访问不频繁】存储到磁盘中。